{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n",
      "Requirement already satisfied: torch==1.13.1 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision==0.14.1 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (0.14.1)\n",
      "Requirement already satisfied: diffusers==0.18.2 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (0.18.2)\n",
      "Requirement already satisfied: scikit-image==0.19.3 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (0.19.3)\n",
      "Requirement already satisfied: scikit-video==1.1.11 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (1.1.11)\n",
      "Requirement already satisfied: zarr==2.12.0 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (2.12.0)\n",
      "Requirement already satisfied: numcodecs==0.10.2 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (0.10.2)\n",
      "Requirement already satisfied: pygame==2.1.2 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: pymunk==6.2.1 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (6.2.1)\n",
      "Requirement already satisfied: gym==0.26.2 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: shapely==1.8.4 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (1.8.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from torch==1.13.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from torch==1.13.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from torch==1.13.1) (8.5.0.96)\n",
      "Requirement already satisfied: typing-extensions in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from torch==1.13.1) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from torch==1.13.1) (11.10.3.66)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from torchvision==0.14.1) (10.3.0)\n",
      "Requirement already satisfied: numpy in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from torchvision==0.14.1) (1.26.4)\n",
      "Requirement already satisfied: requests in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from torchvision==0.14.1) (2.32.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from diffusers==0.18.2) (7.1.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.2 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from diffusers==0.18.2) (0.23.4)\n",
      "Requirement already satisfied: filelock in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from diffusers==0.18.2) (3.15.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from diffusers==0.18.2) (2024.5.15)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (2024.6.18)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (2.34.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (1.6.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (1.13.1)\n",
      "Requirement already satisfied: networkx>=2.2 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (24.1)\n",
      "Requirement already satisfied: fasteners in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from zarr==2.12.0) (0.19)\n",
      "Requirement already satisfied: asciitree in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from zarr==2.12.0) (0.3.3)\n",
      "Requirement already satisfied: entrypoints in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from numcodecs==0.10.2) (0.4)\n",
      "Requirement already satisfied: cffi>=1.15.0 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from pymunk==6.2.1) (1.16.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from gym==0.26.2) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from gym==0.26.2) (3.0.0)\n",
      "Requirement already satisfied: wheel in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.43.0)\n",
      "Requirement already satisfied: setuptools in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (59.6.0)\n",
      "Requirement already satisfied: pycparser in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from cffi>=1.15.0->pymunk==6.2.1) (2.22)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.18.2) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.18.2) (2024.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.18.2) (6.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from importlib-metadata->diffusers==0.18.2) (3.19.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from requests->torchvision==0.14.1) (2024.6.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from requests->torchvision==0.14.1) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from requests->torchvision==0.14.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abhi2001/SRA/venv/lib/python3.10/site-packages (from requests->torchvision==0.14.1) (3.7)\n"
     ]
    }
   ],
   "source": [
    "#@markdown ### **Installing pip packages**\n",
    "#@markdown - Diffusion Model: [PyTorch](https://pytorch.org) & [HuggingFace diffusers](https://huggingface.co/docs/diffusers/index)\n",
    "#@markdown - Dataset Loading: [Zarr](https://zarr.readthedocs.io/en/stable/) & numcodecs\n",
    "#@markdown - Push-T Env: gym, pygame, pymunk & shapely\n",
    "!python --version\n",
    "!pip3 install torch==1.13.1 torchvision==0.14.1 diffusers==0.18.2 \\\n",
    "scikit-image==0.19.3 scikit-video==1.1.11 zarr==2.12.0 numcodecs==0.10.2 \\\n",
    "pygame==2.1.2 pymunk==6.2.1 gym==0.26.2 shapely==1.8.4 \\\n",
    "# &> /dev/null # mute output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhi2001/SRA/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#@markdown ### **Imports**\n",
    "# diffusion policy import\n",
    "from typing import Tuple, Sequence, Dict, Union, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "# import zarr\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# # env import\n",
    "# import gym\n",
    "# from gym import spaces\n",
    "# import pygame\n",
    "# import pymunk\n",
    "# import pymunk.pygame_util\n",
    "# from pymunk.space_debug_draw_options import SpaceDebugColor\n",
    "# from pymunk.vec2d import Vec2d\n",
    "# import shapely.geometry as sg\n",
    "# import cv2\n",
    "# import skimage.transform as st\n",
    "# from skvideo.io import vwrite\n",
    "# from IPython.display import Video\n",
    "# import gdown\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define exo_state dataset with utility functions\n",
    "\n",
    "class ExoStateDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A class to prepare and load the exo state-based action diffusion policy dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file_path_1: str,\n",
    "                csv_file_path_2: str,\n",
    "                episode_stats: dict[str, list],\n",
    "                obs_horizon: int = 10,\n",
    "                pred_horizon: int = 10,\n",
    "                decimation_factor = 1,\n",
    "                sampling_rate = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            csv_file_path_1 (str): The path to the csv file containing the patient dataset.\n",
    "            csv_file_path_2 (str): The path to the csv file containing the instructor dataset.\n",
    "            episode_stats (dict[str, list]): A dictionary containing the episode stats (start and end indices).\n",
    "            obs_horizon (int): The observation horizon.\n",
    "            pred_horizon (int): The prediction horizon.\n",
    "            decimation_factor (int): The decimation factor for the dataset. Reduces the size of the dataset.\n",
    "            sampling_rate (float): The number of seconds to collect (obs_horizon) number of observations.\n",
    "        \"\"\"\n",
    "        # Load only the required decimated data\n",
    "        # combine data from the two csv files\n",
    "        self.data, self.episode_lengths = self.load_and_combine_data(csv_file_path_1, csv_file_path_2, episode_stats, decimation_factor)\n",
    "\n",
    "        # compute the statictics for normalization\n",
    "        self.stats = self.get_data_stats(self.data)\n",
    "\n",
    "        # normalize the data\n",
    "        self.norm_data = self.normalize_data(self.data, self.stats)\n",
    "\n",
    "        # determine sample interval from the sampling rate and decimation factor\n",
    "        actual_data_frequency = 333 # Hz (from the dataset - determined empirically)\n",
    "        # actual_data_frequency is the frequency at which the data was collected\n",
    "        self.sample_interval = int((actual_data_frequency * sampling_rate) / (decimation_factor*obs_horizon)) - 1 # interval between samples\n",
    "        \n",
    "        # create sample indices\n",
    "        sequence_length = obs_horizon + pred_horizon\n",
    "        self.indices = self.create_sample_indices(sequence_length, self.sample_interval)\n",
    "\n",
    "\n",
    "        self.obs_horizon = obs_horizon\n",
    "        self.pred_horizon = pred_horizon\n",
    "\n",
    "    def load_and_combine_data(self, csv_file_path_1: str, csv_file_path_2: str, episode_stats: dict[str, list], decimation_factor: int = 1):\n",
    "        \"\"\"\n",
    "        Load, decimate and combine data from the two csv files.\n",
    "        \"\"\"\n",
    "\n",
    "        chunks = []\n",
    "        episode_lengths = []\n",
    "\n",
    "        for start, end in zip(episode_stats[\"start\"], episode_stats[\"end\"]):\n",
    "            # Load the required chunk of data and apply decimation in one step\n",
    "            # remove first row (header) and 1st column (time)\n",
    "            chunk_1 = pd.read_csv(csv_file_path_1, \n",
    "                                    skiprows = lambda x: x < start or (x - start) % decimation_factor != 0,\n",
    "                                    nrows = (end - start) // decimation_factor + 1,\n",
    "                                    usecols = [1,2,3,4])\n",
    "                                  \n",
    "            chunk_2 = pd.read_csv(csv_file_path_2, \n",
    "                                    skiprows = lambda x: x < start or (x - start) % decimation_factor != 0,\n",
    "                                    nrows = (end - start) // decimation_factor + 1,\n",
    "                                    usecols = [1,2,3,4])\n",
    "\n",
    "            # confirm that the two chunks have the same length\n",
    "            assert len(chunk_1) == len(chunk_2)\n",
    "\n",
    "            # horizontal concatenation\n",
    "            chunk = pd.concat([chunk_1, chunk_2], axis=1, ignore_index=True)\n",
    "\n",
    "            # confirm that the chunk has the correct length\n",
    "            assert len(chunk) == len(chunk_1)\n",
    "\n",
    "            # Append the chunk to the list\n",
    "            chunks.append(chunk)\n",
    "            episode_lengths.append(len(chunk))\n",
    "\n",
    "        # Combine the chunks into a single dataframe\n",
    "        data = pd.concat(chunks, axis=0, ignore_index=True)\n",
    "\n",
    "        return data, episode_lengths\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_data_stats(data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Compute the min and max values of the given dataset.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The dataset.       \n",
    "        \"\"\"\n",
    "\n",
    "        return {\n",
    "            \"min\": np.min(data, axis=0),\n",
    "            \"max\": np.max(data, axis=0),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_data(data: pd.DataFrame, stats: dict):\n",
    "        \"\"\"\n",
    "        Normalize the given dataset.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The dataset.\n",
    "            stats (dict): The statistics of the dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        # normalize to [0, 1]\n",
    "        ndata = (data - stats[\"min\"]) / (stats[\"max\"] - stats[\"min\"])\n",
    "        # normalize to [-1, 1]\n",
    "        ndata = 2 * ndata - 1\n",
    "\n",
    "        return ndata\n",
    "\n",
    "    @staticmethod\n",
    "    def unnormalize_data(ndata: pd.DataFrame, stats: dict):\n",
    "        \"\"\"\n",
    "        Unnormalize the given dataset.\n",
    "\n",
    "        Args:\n",
    "            ndata (pd.DataFrame): The normalized dataset.\n",
    "            stats (dict): The statistics of the dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        # unnormalize to [0, 1]\n",
    "        data = (ndata + 1) / 2\n",
    "        # unnormalize to original range\n",
    "        data = data * (stats[\"max\"] - stats[\"min\"]) + stats[\"min\"]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def create_sample_indices(self, sequence_length: int, sample_int: int):\n",
    "        \"\"\"\n",
    "        Create sample indices.\n",
    "\n",
    "        Args:\n",
    "            sequence_length (int): The sequence length.\n",
    "            sample_int (int): The sample interval.\n",
    "        \"\"\"\n",
    "\n",
    "        indices = []\n",
    "        current_index = 0\n",
    "\n",
    "        for episode_length in self.episode_lengths:\n",
    "            for i in range(episode_length - sample_int * sequence_length + 1):\n",
    "                buffer_start_idx = current_index + i\n",
    "                indices.append(buffer_start_idx)\n",
    "\n",
    "            current_index += episode_length\n",
    "\n",
    "        return np.array(indices)\n",
    "    \n",
    "    def sample_example_data(self):\n",
    "        \"\"\"\n",
    "        Sample example data for testing.\n",
    "        \"\"\"\n",
    "\n",
    "        # sample an index\n",
    "        idx = np.random.randint(0, len(self))\n",
    "\n",
    "        return self[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get the item at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index.\n",
    "        \"\"\"\n",
    "\n",
    "        buffer_start_idx = self.indices[idx]\n",
    "\n",
    "        # fetch the observation and prediction data\n",
    "        # each sample row must be seperated from the next by the sample interval\n",
    "        # for observation data use the normalized dataset\n",
    "        obs_data = self.norm_data.iloc[buffer_start_idx:buffer_start_idx + self.obs_horizon*self.sample_interval:self.sample_interval].values\n",
    "\n",
    "        # for prediction data, only fetch the last 4 values from the original dataset\n",
    "        # these values correspond to the instructor joint position data\n",
    "        pred_data = self.data.iloc[buffer_start_idx + self.obs_horizon*self.sample_interval:buffer_start_idx + (self.obs_horizon + self.pred_horizon)*self.sample_interval:self.sample_interval, -4:].values\n",
    "\n",
    "        # convert the pred_data from radians to degrees\n",
    "        pred_data = np.degrees(pred_data)\n",
    "\n",
    "        return obs_data, pred_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 10, 8]) torch.Size([256, 20, 4])\n",
      "length of train dataset: 80190\n"
     ]
    }
   ],
   "source": [
    "# Dataset Demo\n",
    "\n",
    "# parameters\n",
    "observation_horizon = 10\n",
    "prediction_horizon = 20\n",
    "decimation_factor = 5\n",
    "sampling_rate = 1.5\n",
    "\n",
    "#...|o|o|                     observations: 10\n",
    "#   | | |a|a|a|a|...          actions executed: (can be any number)\n",
    "#   | | |p|p|p|p|p|p|p|p|p|p| actions predicted: 10\n",
    "\n",
    "\n",
    "# load the train dataset\n",
    "csv_file_path_1 = \"./data/X2_SRA_A_07-05-2024_10-39-10-mod-sync.csv\"\n",
    "csv_file_path_2 = \"./data/X2_SRA_B_07-05-2024_10-41-46-mod-sync.csv\"\n",
    "# episode stats specified in row indices\n",
    "# corresponds to the values (in secs) given below\n",
    "#   \"start\": [795,1795]\n",
    "#   \"end\": [1405,2395]\n",
    "episode_stats = {\n",
    "    \"start\": [164041,454139],\n",
    "    \"end\": [367374,654139]\n",
    "}\n",
    "\n",
    "train_dataset = ExoStateDataset(csv_file_path_1, csv_file_path_2, episode_stats,\n",
    "                                observation_horizon, prediction_horizon, decimation_factor, sampling_rate)\n",
    "\n",
    "# create a dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, \n",
    "                                           num_workers=1, pin_memory=True,\n",
    "                                           persistent_workers=True,\n",
    "                                           shuffle=True)\n",
    "\n",
    "# visualize the dataset\n",
    "batch = next(iter(train_loader))\n",
    "obs_data, pred_data = batch\n",
    "print(obs_data.shape, pred_data.shape)\n",
    "print(\"length of train dataset:\", len(train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation data shape: (10, 8)\n",
      "sampled observation data: [[-0.57288916  0.89971604  0.61477838 -0.13752532  0.68796385 -0.12582482\n",
      "  -0.27560398  0.9335978 ]\n",
      " [-0.62073077  0.91800788  0.32523872  0.27638096  0.43316597  0.258925\n",
      "  -0.34057954  0.93956195]\n",
      " [-0.6876724   0.92924872 -0.04904166  0.68976765  0.10887955  0.60974226\n",
      "  -0.4224003   0.93967712]\n",
      " [-0.72891278  0.93297951 -0.43367806  0.93848297 -0.2591415   0.88438462\n",
      "  -0.49205147  0.94362632]\n",
      " [-0.78510478  0.91078123 -0.65160511  0.92266587 -0.52122402  0.92831682\n",
      "  -0.60131627  0.90075782]\n",
      " [-0.88285886  0.90649654 -0.64882046  0.87975759 -0.60340704  0.87927964\n",
      "  -0.76759626  0.86414947]\n",
      " [-0.91108443  0.92519357 -0.64505588  0.85015582 -0.61582403  0.87183509\n",
      "  -0.9042838   0.83058079]\n",
      " [-0.90183935  0.88242318 -0.6494342   0.86562242 -0.63418369  0.89215506\n",
      "  -0.89871805  0.73406074]\n",
      " [-0.83490555  0.73601414 -0.67606234  0.87748552 -0.65097403  0.89726629\n",
      "  -0.823352    0.51148188]\n",
      " [-0.73348673  0.44144274 -0.70101125  0.86600875 -0.68013706  0.89652912\n",
      "  -0.70002723  0.16633595]]\n",
      "prediction data shape: (20, 4)\n",
      "sampled prediction data: [[ -10.90132419   -8.07268885   -1.83850697  -71.95656628]\n",
      " [ -11.29162304   -7.37849319   12.27923039  -89.63156941]\n",
      " [  -8.78155224   -7.79497621   28.40730478 -100.67601846]\n",
      " [  -5.43811432   -7.92847538   45.62061852 -106.04595081]\n",
      " [  -2.79563297   -7.94211177   58.65139766 -104.02518596]\n",
      " [  -1.0643837    -7.95569087   62.4668955   -92.80678692]\n",
      " [  -0.42943187   -8.41921373   58.1742384   -74.04660172]\n",
      " [  -0.68090304   -8.95406983   48.94394562  -52.64542486]\n",
      " [  -2.16606694   -9.01113643   38.9743081   -33.95410919]\n",
      " [  -4.56171808   -9.01004781   29.40533996  -20.77269945]\n",
      " [  -7.73939931   -8.93040667   20.95455626  -11.29162304]\n",
      " [  -7.12593339   -9.65611502   10.17166244   -8.93791242]\n",
      " [ -12.37892505  -14.78500402    9.79998472   -8.14442317]\n",
      " [ -15.14625391  -19.74779255   11.64101271  -10.20248757]\n",
      " [ -13.42932858  -29.02936506   10.04824733   -9.2328138 ]\n",
      " [  -8.78584942  -47.74864107    7.82047283   -8.62261375]\n",
      " [   0.75871071  -69.27684267    5.7343908    -8.77971877]\n",
      " [  13.41317117  -88.38389652    5.3989813    -8.42316714]\n",
      " [  28.59999048 -101.05554571    6.77333517   -8.47553548]\n",
      " [  46.47512778 -105.68005996    8.11715038   -8.64845414]]\n"
     ]
    }
   ],
   "source": [
    "# sample example data\n",
    "obs_data, pred_data = train_dataset.sample_example_data()\n",
    "print(\"observation data shape:\", obs_data.shape)\n",
    "print(\"sampled observation data:\", obs_data)\n",
    "print(\"prediction data shape:\", pred_data.shape)\n",
    "print(\"sampled prediction data:\", pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture for Diffusion Model\n",
    "\n",
    "# Defines a 1D UNet architecture \"ConditionalUnet1D\" as the noise prediction network.\n",
    "\n",
    "# Components: \n",
    "# - 'SinusoidalPosEmb' Positional encoding for the diffusion iteration k.\n",
    "# - 'Downsampled' Strided convolution to reduce temporal resolution.\n",
    "# - 'Upsampled' Transposed convolution to increase temporal resolution.\n",
    "# - 'Conv1dBlock' Conv1d --> GroupNorm --> Mish\n",
    "# - 'ConditionalResidualBlock1D' Takes two inputs 'x' and 'cond'.\n",
    "#    x is passes through 2 'Conv1dblock' stacked together with residual connection.\n",
    "#    'cond' is applied to 'x' with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "        return emb\n",
    "    \n",
    "class Downsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Conv1dBlock(nn.Module):\n",
    "    '''\n",
    "        Conv1d --> GroupNorm --> Mish\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.GroupNorm(n_groups, out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ConditionalResidualBlock1D(nn.Module):\n",
    "    def __init__(self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            cond_dim,\n",
    "            kernel_size=3,\n",
    "            n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "        ])\n",
    "\n",
    "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
    "        # predicts per-channel scale and bias\n",
    "        cond_channels = out_channels * 2\n",
    "        self.out_channels = out_channels\n",
    "        self.cond_encoder = nn.Sequential(\n",
    "            nn.Mish(),\n",
    "            nn.Linear(cond_dim, cond_channels),\n",
    "            nn.Unflatten(-1, (-1, 1))\n",
    "        )\n",
    "\n",
    "        # make sure dimensions compatible\n",
    "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        '''\n",
    "            x : [ batch_size x in_channels x horizon ]\n",
    "            cond : [ batch_size x cond_dim]\n",
    "\n",
    "            returns:\n",
    "            out : [ batch_size x out_channels x horizon ]\n",
    "        '''\n",
    "        out = self.blocks[0](x)\n",
    "        embed = self.cond_encoder(cond)\n",
    "\n",
    "        embed = embed.reshape(\n",
    "            embed.shape[0], 2, self.out_channels, 1)\n",
    "        scale = embed[:,0,...]\n",
    "        bias = embed[:,1,...]\n",
    "        out = scale * out + bias\n",
    "\n",
    "        out = self.blocks[1](out)\n",
    "        out = out + self.residual_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConditionalUnet1D(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_dim,\n",
    "        global_cond_dim,\n",
    "        diffusion_step_embed_dim=256,\n",
    "        down_dims=[256,512,1024],\n",
    "        kernel_size=5,\n",
    "        n_groups=8\n",
    "        ):\n",
    "        \"\"\"\n",
    "        input_dim: Dim of actions.\n",
    "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
    "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
    "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
    "        down_dims: Channel size for each UNet level.\n",
    "          The length of this array determines numebr of levels.\n",
    "        kernel_size: Conv kernel size\n",
    "        n_groups: Number of groups for GroupNorm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        all_dims = [input_dim] + list(down_dims)\n",
    "        start_dim = down_dims[0]\n",
    "\n",
    "        dsed = diffusion_step_embed_dim\n",
    "        diffusion_step_encoder = nn.Sequential(\n",
    "            SinusoidalPosEmb(dsed),\n",
    "            nn.Linear(dsed, dsed * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(dsed * 4, dsed),\n",
    "        )\n",
    "        cond_dim = dsed + global_cond_dim\n",
    "\n",
    "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
    "        mid_dim = all_dims[-1]\n",
    "        self.mid_modules = nn.ModuleList([\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        down_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            down_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        up_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            up_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        final_conv = nn.Sequential(\n",
    "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
    "            nn.Conv1d(start_dim, input_dim, 1),\n",
    "        )\n",
    "\n",
    "        self.diffusion_step_encoder = diffusion_step_encoder\n",
    "        self.up_modules = up_modules\n",
    "        self.down_modules = down_modules\n",
    "        self.final_conv = final_conv\n",
    "\n",
    "        print(\"number of parameters: {:e}\".format(\n",
    "            sum(p.numel() for p in self.parameters()))\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "            sample: torch.Tensor,\n",
    "            timestep: Union[torch.Tensor, float, int],\n",
    "            global_cond=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,input_dim)\n",
    "        timestep: (B,) or int, diffusion step\n",
    "        global_cond: (B,global_cond_dim)\n",
    "        output: (B,T,input_dim)\n",
    "        \"\"\"\n",
    "        # (B,T,C)\n",
    "        sample = sample.moveaxis(-1,-2)\n",
    "        # (B,C,T)\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        global_feature = self.diffusion_step_encoder(timesteps)\n",
    "\n",
    "        if global_cond is not None:\n",
    "            global_feature = torch.cat([\n",
    "                global_feature, global_cond\n",
    "            ], axis=-1)\n",
    "\n",
    "        x = sample\n",
    "        h = []\n",
    "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        for mid_module in self.mid_modules:\n",
    "            x = mid_module(x, global_feature)\n",
    "\n",
    "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # (B,C,T)\n",
    "        x = x.moveaxis(-1,-2)\n",
    "        # (B,T,C)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 6.636032e+07\n"
     ]
    }
   ],
   "source": [
    "# Network Demo\n",
    "\n",
    "# parameters\n",
    "obs_dim = 8 # for joint positions (patient + instructor)\n",
    "action_dim = 4 # for instructor joint positions\n",
    "\n",
    "# create network object\n",
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim * observation_horizon,\n",
    ")\n",
    "\n",
    "# example inputs\n",
    "noised_action = torch.randn((1, prediction_horizon, action_dim))\n",
    "obs = torch.zeros((1, observation_horizon, obs_dim))\n",
    "diffusion_iter = torch.zeros((1,))\n",
    "\n",
    "# the noise prediction network\n",
    "# takes the noisy action, diffusion iteration and observation as input\n",
    "# predicts the noise added to action\n",
    "noise = noise_pred_net(\n",
    "    sample=noised_action,\n",
    "    timestep=diffusion_iter,\n",
    "    global_cond=obs.flatten(start_dim=1)\n",
    ")\n",
    "\n",
    "# illustration of removing noise\n",
    "# the actual noise removal is performed by NoiseScheduler\n",
    "# and is dependent on the diffusion noise schedule\n",
    "denoised_action = noised_action - noise\n",
    "\n",
    "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
    "num_diffusion_iters = 100\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    # the choice of beta schedule has big impact on performace\n",
    "    # we found squared cosine works the best\n",
    "    beta_schedule=\"squaredcos_cap_v2\",\n",
    "    # clip output to [-180,180] to improve stability\n",
    "    clip_sample=True,\n",
    "    clip_sample_range=180.0,\n",
    "    # our network predicts noise (instead of denoised action)\n",
    "    prediction_type=\"epsilon\"\n",
    ")\n",
    "\n",
    "# device transfer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "_ = noise_pred_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noised Action Shape: torch.Size([1, 20, 4])\n",
      "Diffusion Iter Shape: torch.Size([1])\n",
      "Flattened Obs Shape: torch.Size([1, 80])\n",
      "Predicted Noise Shape: torch.Size([1, 20, 4])\n",
      "Denoised Action Shape: torch.Size([1, 20, 4])\n"
     ]
    }
   ],
   "source": [
    "# Print shapes of input tensors\n",
    "# Flattening the observation tensor\n",
    "flattened_obs = obs.flatten(start_dim=1)\n",
    "print(f\"Noised Action Shape: {noised_action.shape}\")\n",
    "print(f\"Diffusion Iter Shape: {diffusion_iter.shape}\")\n",
    "print(f\"Flattened Obs Shape: {flattened_obs.shape}\")\n",
    "print(f\"Predicted Noise Shape: {noise.shape}\")\n",
    "print(f\"Denoised Action Shape: {denoised_action.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "Train Dataset Length: 80190\n",
      "Train Loader Length: 314\n",
      "Test Dataset Length: 39428\n",
      "Test Loader Length: 78\n"
     ]
    }
   ],
   "source": [
    "# ***Training the Action Diffusion Model***\n",
    "# preparing the data\n",
    "\n",
    "# parameters\n",
    "observation_horizon = 10\n",
    "prediction_horizon = 20\n",
    "decimation_factor = 5\n",
    "sampling_rate = 1.5\n",
    "\n",
    "# load the train dataset\n",
    "csv_file_path_1 = \"./data/X2_SRA_A_07-05-2024_10-39-10-mod-sync.csv\"\n",
    "csv_file_path_2 = \"./data/X2_SRA_B_07-05-2024_10-41-46-mod-sync.csv\"\n",
    "\n",
    "# episode stats specified in row indices\n",
    "# corresponds to the values (in secs) given below\n",
    "#   \"start\": [795,1795]\n",
    "#   \"end\": [1405,2395]\n",
    "episode_stats_train = {\n",
    "    \"start\": [164041,454139],\n",
    "    \"end\": [367374,654139]\n",
    "}\n",
    "\n",
    "# episode stats specified in row indices\n",
    "# corresponds to the values (in secs) given below\n",
    "#   \"start\": [2880]\n",
    "#   \"end\": [3475]\n",
    "episode_stats_test = {\n",
    "    \"start\": [696306],\n",
    "    \"end\": [894640]\n",
    "}\n",
    "\n",
    "# intialize fresh train and test datasets\n",
    "train_dataset = ExoStateDataset(csv_file_path_1, csv_file_path_2, episode_stats_train,\n",
    "                                observation_horizon, prediction_horizon, decimation_factor,\n",
    "                                sampling_rate)\n",
    "test_dataset = ExoStateDataset(csv_file_path_1, csv_file_path_2, episode_stats_test,\n",
    "                                observation_horizon, prediction_horizon, decimation_factor,\n",
    "                                sampling_rate)\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256,\n",
    "                                            num_workers=1, pin_memory=True,\n",
    "                                            persistent_workers=True,\n",
    "                                            shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512,\n",
    "                                            num_workers=1, pin_memory=True,\n",
    "                                            persistent_workers=True,\n",
    "                                            shuffle=True)\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Train Dataset Length: {len(train_dataset)}\")\n",
    "print(f\"Train Loader Length: {len(train_loader)}\")\n",
    "print(f\"Test Dataset Length: {len(test_dataset)}\")\n",
    "print(f\"Test Loader Length: {len(test_loader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an infinite iterator to fetch random samples\n",
    "# from the test dataset\n",
    "# this is done to avoid running out of samples during long training runs\n",
    "# we only test on a subset of samples from the test dataset which is\n",
    "# different from the standard practice of testing on the entire test dataset\n",
    "\n",
    "def infinite_data_loader(dataloader):\n",
    "    \"\"\"\n",
    "    Create an infinite iterator for the given dataloader.\n",
    "\n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader.\n",
    "    \"\"\"\n",
    "\n",
    "    while True:\n",
    "        for data in dataloader:\n",
    "            yield data\n",
    "\n",
    "\n",
    "# create infinite iterators for the test dataloader\n",
    "test_iter = infinite_data_loader(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation\n",
    "# training parameters\n",
    "num_epochs = 1000\n",
    "\n",
    "# Exponential Moving Average\n",
    "# accelerates training and improves stability\n",
    "# holds a copy of the model weights\n",
    "ema = EMAModel(\n",
    "    parameters=noise_pred_net.parameters(),\n",
    "    power=0.75)\n",
    "\n",
    "# Standard ADAM optimizer\n",
    "# Note that EMA parametesr are not optimized\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=noise_pred_net.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(train_loader) * num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Noise: -0.07305299490690231\n",
      "Time Taken for a single inference: 0:00:02.113826\n",
      "Loss: 25467.802734375\n",
      "Predicted Action: tensor([[[ 114.0684, -179.9731, -128.9939,  180.0000],\n",
      "         [ 163.1590,  175.0827, -178.6961,  179.4155],\n",
      "         [ 148.4457, -159.3444,  167.8218,  178.3703],\n",
      "         [-172.3243,  178.2047,  159.1896,  180.0000],\n",
      "         [-161.5103,   48.4356,  177.1535,  180.0000],\n",
      "         [-158.8526,  -98.6993,  144.4286,  175.6401],\n",
      "         [ 175.7877,  177.2434,  110.1243,  165.7906],\n",
      "         [ 112.2866, -174.8158, -147.7071,  -51.9838],\n",
      "         [ 116.4549, -178.2416,  179.3771,   85.3674],\n",
      "         [ 180.0000, -145.8908,   23.3472, -164.9433],\n",
      "         [-100.1914, -163.7776,  116.4733,  140.9861],\n",
      "         [-131.5169,  171.9418, -177.7422,  179.8164],\n",
      "         [ 179.9819,  178.3191,  153.8547,  179.5425],\n",
      "         [ 173.6896,  104.1027,  -91.3391,  -95.9202],\n",
      "         [-180.0000, -132.5854,  167.1690,  112.4017],\n",
      "         [ 178.1696,  177.4347,  171.6521,  169.4555],\n",
      "         [ 113.1582,   -8.3925,   12.8773,  179.3808],\n",
      "         [ 172.5796,  173.9099,  179.8808,  141.5249],\n",
      "         [ 179.4340,   53.7319,  -12.5491,  -37.2612],\n",
      "         [ 176.8058,  159.4317,  -25.5726,  -75.8920]]], device='cuda:0')\n",
      "Actual Action: tensor([[[  -2.5969,   -8.4608,   58.8024,  -95.2376],\n",
      "         [  -2.7321,   -8.9494,   57.8033,  -79.4732],\n",
      "         [  -3.6265,   -9.3207,   50.5827,  -58.8279],\n",
      "         [  -5.3857,   -9.3257,   38.9305,  -39.4141],\n",
      "         [  -7.6827,   -9.3200,   26.2980,  -24.6104],\n",
      "         [  -9.4563,   -9.3691,   15.0612,  -12.8058],\n",
      "         [ -10.1673,  -11.0925,    3.4001,   -7.6823],\n",
      "         [ -15.7091,  -15.6757,    2.1965,   -8.5458],\n",
      "         [ -19.4870,  -17.8106,    3.2595,  -10.8443],\n",
      "         [ -18.7759,  -21.7169,    1.8393,  -10.2387],\n",
      "         [ -14.8453,  -30.2376,   -0.7971,   -9.4477],\n",
      "         [  -9.6231,  -47.1097,   -1.2326,   -9.8649],\n",
      "         [   1.5562,  -72.1772,   -1.7567,   -9.6425],\n",
      "         [  16.9901,  -91.3528,   -0.8477,   -9.0456],\n",
      "         [  34.9841, -104.3050,    1.3413,   -9.0789],\n",
      "         [  50.8054, -107.6423,    3.4722,   -9.0800],\n",
      "         [  60.0792,  -97.9259,    3.7850,   -9.0588],\n",
      "         [  56.4448,  -77.7029,    3.4596,   -9.0803],\n",
      "         [  46.4066,  -55.3180,    0.7433,   -9.0692],\n",
      "         [  33.7740,  -34.5152,   -2.1237,   -9.0559]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# before training, produce a sample prediction to check if the model is working\n",
    "# and benchmark the time taken, loss values etc.\n",
    "import datetime as dt\n",
    "# take an example observation from the train dataset\n",
    "obs_data, pred_data = train_dataset.sample_example_data()\n",
    "\n",
    "# convert to tensor\n",
    "obs_data = torch.tensor(obs_data, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "pred_data = torch.tensor(pred_data, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "noise_pred_net.eval()\n",
    "with torch.no_grad():    \n",
    "    # flatten the observation tensor\n",
    "    obs_cond = obs_data.flatten(start_dim=1)\n",
    "\n",
    "    # inititalize the action from Guassian noise\n",
    "    noisy_action = (torch.randn((1, prediction_horizon, action_dim))).to(device)\n",
    "    naction = noisy_action\n",
    "\n",
    "    # init scheduler\n",
    "    noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "\n",
    "    # run the model\n",
    "    noise_mean = list()\n",
    "    # start the timer\n",
    "    start_time = dt.datetime.now()\n",
    "    for k in noise_scheduler.timesteps:\n",
    "    # predict noise\n",
    "        noise_pred = noise_pred_net(\n",
    "            sample=naction,\n",
    "            timestep=k,\n",
    "            global_cond=obs_cond\n",
    "        )\n",
    "\n",
    "        # calculate the mean of the noise\n",
    "        noise_pred_mean = noise_pred.mean(dim=0, keepdim=True)\n",
    "        noise_mean.append(noise_pred_mean)\n",
    "\n",
    "        # inverse diffusion step (remove noise)\n",
    "        naction = noise_scheduler.step(\n",
    "            model_output=noise_pred,\n",
    "            timestep=k,\n",
    "            sample=naction\n",
    "        ).prev_sample\n",
    "\n",
    "    print(f\"Mean Noise: {torch.stack(noise_mean).mean()}\")\n",
    "    # end the timer\n",
    "    end_time = dt.datetime.now()\n",
    "\n",
    "# calculate the time taken\n",
    "time_taken = end_time - start_time\n",
    "print(f\"Time Taken for a single inference: {time_taken}\")\n",
    "\n",
    "# calculate the loss\n",
    "loss = nn.functional.mse_loss(naction, pred_data)\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "# print both tensors\n",
    "print(f\"Predicted Action: {naction}\")\n",
    "print(f\"Actual Action: {pred_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "if train:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "    # initialize tensorboard writer\n",
    "    writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: load the model weights from a previous run\n",
    "load_pretrained = True\n",
    "if load_pretrained:\n",
    "    noise_pred_net.load_state_dict(torch.load(\"ema_noise_pred_net.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    # training loop with validation done at the end of each epoch\n",
    "    train_loss = list()\n",
    "    test_loss = list()\n",
    "\n",
    "    noise_pred_net.train()\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        print(f\"Current Epoch: {epoch_idx}\")\n",
    "        epoch_train_loss = list()\n",
    "\n",
    "        with tqdm(train_loader, desc='Batch', leave=False) as tepoch:\n",
    "            for nbatch in tepoch:\n",
    "                # data normalized in dataset\n",
    "                # device transfer\n",
    "                nobs = nbatch[0].float().to(device)\n",
    "                npred = nbatch[1].float().to(device)\n",
    "                B = nobs.shape[0]\n",
    "\n",
    "                # observation as FiLM conditioning\n",
    "                # (B, obs_horizon, obs_dim)\n",
    "                obs_cond = nobs[:,:observation_horizon,:]\n",
    "                # (B, obs_horizon * obs_dim)\n",
    "                obs_cond = obs_cond.flatten(start_dim=1)\n",
    "\n",
    "                # sample noise to add to actions\n",
    "                noise = torch.randn(npred.shape, device=device, dtype=torch.float)\n",
    "\n",
    "                # sample a diffusion iteration for each data point\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps,\n",
    "                    (B,), device=device\n",
    "                ).long()\n",
    "\n",
    "                # add noise to the clean samples according to the noise magnitude at each diffusion iteration\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_actions = noise_scheduler.add_noise(\n",
    "                    npred, noise, timesteps)\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = noise_pred_net(\n",
    "                    noisy_actions, timesteps, global_cond=obs_cond)\n",
    "                \n",
    "                # L2 loss\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "                # optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # step lr scheduler every batch\n",
    "                # this is different from standard pytorch behavior\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # update Exponential Moving Average of the model weights\n",
    "                ema.step(noise_pred_net.parameters())\n",
    "\n",
    "                # logging\n",
    "                loss_cpu = loss.item()\n",
    "                epoch_train_loss.append(loss_cpu)\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "\n",
    "        # print average training loss for the epoch\n",
    "        print(f\"Epoch {epoch_idx} Training Loss: {np.mean(epoch_train_loss)}\")\n",
    "        train_loss.append(np.mean(epoch_train_loss))\n",
    "        writer.add_scalar('Loss/train', np.mean(epoch_train_loss), epoch_idx)\n",
    "\n",
    "        # Weights of the EMA model\n",
    "        # is used for inference\n",
    "        ema_noise_pred_net = noise_pred_net\n",
    "        ema.copy_to(ema_noise_pred_net.parameters())\n",
    "\n",
    "        # save the model weights\n",
    "        # rewrite the same file\n",
    "        torch.save(ema_noise_pred_net.state_dict(), \"ema_noise_pred_net.pth\")\n",
    "\n",
    "        # validation\n",
    "        # sample a batch from the test dataset and compute loss against it\n",
    "        # use the infinite iterator to avoid running out of samples\n",
    "        tbatch = next(test_iter)\n",
    "\n",
    "        nobs = tbatch[0].clone().detach().float().to(device)\n",
    "        npred = tbatch[1].clone().detach().float().to(device)\n",
    "        B = nobs.shape[0]\n",
    "\n",
    "        # inference\n",
    "        with torch.no_grad():\n",
    "            obs_cond = nobs[:,:observation_horizon,:]\n",
    "            obs_cond = obs_cond.flatten(start_dim=1)\n",
    "\n",
    "            # initialize action from Gaussian noise\n",
    "            noisy_action = torch.randn((B, prediction_horizon, action_dim), device=device)\n",
    "            naction = noisy_action\n",
    "\n",
    "            # init scheduler\n",
    "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "\n",
    "            for k in noise_scheduler.timesteps:\n",
    "                # predict noise\n",
    "                noise_pred = ema_noise_pred_net(\n",
    "                    sample=naction,\n",
    "                    timestep=k,\n",
    "                    global_cond=obs_cond\n",
    "                )\n",
    "\n",
    "                # inverse diffusion step (remove noise)\n",
    "                naction = noise_scheduler.step(\n",
    "                    model_output=noise_pred,\n",
    "                    timestep=k,\n",
    "                    sample=naction\n",
    "                ).prev_sample\n",
    "\n",
    "        # calculate test loss\n",
    "        loss = nn.functional.mse_loss(naction, npred)\n",
    "        loss_cpu = loss.item()\n",
    "        test_loss.append(loss_cpu)\n",
    "        print(f\"Epoch {epoch_idx} Test Loss: {loss_cpu}\\n\")\n",
    "        writer.add_scalar('Loss/test', loss_cpu, epoch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Noise: 0.05821898952126503\n",
      "Time Taken for a single inference: 0:00:01.066993\n",
      "Loss: 128.9589080810547\n",
      "Shape of Predicted Action: (1, 20, 4)\n",
      "Shape of Actual Action: (1, 20, 4)\n",
      "Predicted Action: [[[ -8.71 -35.66  -0.36  -9.89]\n",
      "  [ -2.95 -47.26  -1.17  -9.4 ]\n",
      "  [  6.25 -60.19  -1.61  -8.96]\n",
      "  [ 20.19 -71.2   -1.11  -8.83]\n",
      "  [ 37.2  -75.07   0.31  -8.73]\n",
      "  [ 49.48 -69.08   3.78  -8.55]\n",
      "  [ 51.06 -53.94   6.42  -9.03]\n",
      "  [ 42.63 -35.14   6.1   -9.32]\n",
      "  [ 29.53 -18.97   1.96  -9.2 ]\n",
      "  [ 12.94  -8.07  -1.86  -9.3 ]\n",
      "  [  2.06  -7.37  -7.94 -12.97]\n",
      "  [  0.34  -9.14 -16.31 -19.37]\n",
      "  [ -1.61  -8.89 -16.31 -27.05]\n",
      "  [ -3.13  -7.81 -12.59 -39.86]\n",
      "  [ -4.28  -7.81  -3.39 -56.09]\n",
      "  [ -4.45  -8.     9.2  -71.71]\n",
      "  [ -2.86  -7.67  23.78 -83.5 ]\n",
      "  [ -0.32  -7.7   39.99 -90.  ]\n",
      "  [  2.73  -7.7   53.05 -89.81]\n",
      "  [  5.51  -8.15  59.24 -82.52]]]\n",
      "Actual Action: [[[  -5.67  -57.71    4.53   -9.31]\n",
      "  [   4.85  -78.92    3.75   -9.29]\n",
      "  [  19.03  -97.33    4.25   -9.28]\n",
      "  [  36.76 -106.04    5.36   -9.27]\n",
      "  [  54.55 -103.79    7.09   -9.27]\n",
      "  [  66.48  -90.03    9.39   -9.45]\n",
      "  [  66.75  -69.2    11.98   -9.64]\n",
      "  [  56.19  -47.93   12.93  -10.11]\n",
      "  [  40.65  -28.44   11.93  -10.5 ]\n",
      "  [  25.67  -10.99    7.14  -10.41]\n",
      "  [  14.9    -6.68   -1.28   -8.94]\n",
      "  [   9.04   -8.19  -10.37  -11.65]\n",
      "  [   5.69   -8.46  -17.3   -18.97]\n",
      "  [   0.33   -8.61  -17.2   -37.12]\n",
      "  [  -7.02   -8.1   -11.86  -59.51]\n",
      "  [ -11.27   -8.18   -2.56  -80.3 ]\n",
      "  [ -11.81   -8.34   10.86  -94.39]\n",
      "  [  -9.58   -8.35   27.4  -101.04]\n",
      "  [  -6.91   -8.24   45.58 -101.67]\n",
      "  [  -2.88   -8.09   59.91  -96.58]]]\n"
     ]
    }
   ],
   "source": [
    "# produce an inference to qualitatively test model performance\n",
    "\n",
    "# take an example observation from the test dataset\n",
    "obs_data, pred_data = test_dataset.sample_example_data()\n",
    "\n",
    "# convert to tensor\n",
    "obs_data = torch.tensor(obs_data, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "pred_data = torch.tensor(pred_data, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "noise_pred_net.eval()\n",
    "with torch.no_grad():\n",
    "    # flatten the observation tensor\n",
    "    obs_cond = obs_data.flatten(start_dim=1)\n",
    "\n",
    "    # inititalize the action from Guassian noise\n",
    "    noisy_action = (torch.randn((1, prediction_horizon, action_dim))).to(device)\n",
    "    naction = noisy_action\n",
    "\n",
    "    # init scheduler\n",
    "    noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "\n",
    "    # run the model\n",
    "    noise_mean = list()\n",
    "    # start the timer\n",
    "    start_time = dt.datetime.now()\n",
    "    for k in noise_scheduler.timesteps:\n",
    "        # predict noise\n",
    "        noise_pred = noise_pred_net(\n",
    "            sample=naction,\n",
    "            timestep=k,\n",
    "            global_cond=obs_cond\n",
    "        )\n",
    "\n",
    "        # calculate the mean of the noise\n",
    "        noise_pred_mean = noise_pred.mean(dim=0, keepdim=True)\n",
    "        noise_mean.append(noise_pred_mean)\n",
    "\n",
    "        # inverse diffusion step (remove noise)\n",
    "        naction = noise_scheduler.step(\n",
    "            model_output=noise_pred,\n",
    "            timestep=k,\n",
    "            sample=naction\n",
    "        ).prev_sample\n",
    "\n",
    "    print(f\"Mean Noise: {torch.stack(noise_mean).mean()}\")\n",
    "    # end the timer\n",
    "    end_time = dt.datetime.now()\n",
    "\n",
    "# calculate the time taken\n",
    "time_taken = end_time - start_time\n",
    "print(f\"Time Taken for a single inference: {time_taken}\")\n",
    "\n",
    "# calculate the loss\n",
    "loss = nn.functional.mse_loss(naction, pred_data)\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "# print both tensors\n",
    "naction = naction.cpu().numpy()\n",
    "pred_data = pred_data.cpu().numpy()\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "print(f\"Shape of Predicted Action: {naction.shape}\")\n",
    "print(f\"Shape of Actual Action: {pred_data.shape}\")\n",
    "print(f\"Predicted Action: {naction}\")\n",
    "print(f\"Actual Action: {pred_data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
